{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec259c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import threading\n",
    "import urllib.parse\n",
    "import logging\n",
    "\n",
    "# Proxy settings\n",
    "username = 'user-splx70gleh-country-us'\n",
    "password = 'h4ui90lsvyZOS2chAr'\n",
    "proxy = f\"socks5h://{username}:{password}@gate.smartproxy.com:7000\"\n",
    "proxies = {\n",
    "    'http': proxy,\n",
    "    'https': proxy\n",
    "}\n",
    "\n",
    "# Load the xls file\n",
    "df = pd.read_excel('list_data.xlsx')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraping_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Define a function to generate email guesses\n",
    "def guess_emails(business_name):\n",
    "    # Remove non-alphanumeric characters\n",
    "    clean_name = re.sub(r'\\W+', '', business_name)\n",
    "    return f'{clean_name}@gmail.com, info@{clean_name}.com'\n",
    "\n",
    "# Define a function to search Google\n",
    "def google_search(company_name):\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    url = 'https://www.google.com/search'\n",
    "    try:\n",
    "        response = requests.get(url, params={'q': company_name}, proxies=proxies)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Problem with request: {str(e)}\")\n",
    "        return None\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('div', class_='kCrYT')\n",
    "        first_link = None\n",
    "        for link in links:\n",
    "            a_tag = link.find('a')\n",
    "            if a_tag is None:  \n",
    "                continue\n",
    "            url = a_tag.get('href')  \n",
    "            url = re.sub(r\"/url\\?q=([^&]*)&sa=.*\", r\"\\1\", url)\n",
    "            if any(substring in url.lower() for substring in [\"orginization\", \"company\",\"thebluebook\",\"profile\",\"directory\",\"directories\",\"zoom\",\"info\",\"indeed\",\"vendor\",\"companies\",\"yelp\",\"youtube\", \"google\", \"wikipedia\", \"facebook\", \"instagram\", \"linkedin\", \"twitter\"]):\n",
    "                continue\n",
    "\n",
    "            # Split the company name into words of length greater than 5\n",
    "            words = [word for word in company_name.split() if len(word) > 5]\n",
    "\n",
    "            # Get the URL's domain\n",
    "            domain = urllib.parse.urlparse(url).netloc\n",
    "\n",
    "            # If any word from the company name appears in the domain, choose this URL\n",
    "            if any(word.lower() in domain.lower() for word in words):\n",
    "                return url\n",
    "            if first_link is None:\n",
    "                first_link = url\n",
    "        return first_link if first_link is not None else \"No relevant link found\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Problem parsing response: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define a function to scrape a website for contact information\n",
    "def scrape_website(url):\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    try:\n",
    "        response = requests.get(url, proxies=proxies)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Problem with request: {str(e)}\")\n",
    "        return None\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        phone_pattern = r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
    "        email_matches = re.findall(email_pattern, text)\n",
    "        phone_matches = re.findall(phone_pattern, text)\n",
    "        contact_link = None\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            if \"contact\" in a_tag.text.lower():\n",
    "                contact_link = urllib.parse.urljoin(url, a_tag['href'])\n",
    "                break\n",
    "        return email_matches, phone_matches, contact_link\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Problem parsing response: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Define a wrapper function to call google_search with a timeout\n",
    "def google_search_with_timeout(row, timeout=60):\n",
    "    result = [None]\n",
    "    def job():\n",
    "        result[0] = google_search(row['Legal Business Name'])\n",
    "    thread = threading.Thread(target=job)\n",
    "    thread.start()\n",
    "    thread.join(timeout=timeout)\n",
    "    if thread.is_alive():\n",
    "        logging.error(f\"Timeout: google_search took longer than {timeout} seconds.\")\n",
    "        return None\n",
    "    else:\n",
    "        return result[0]\n",
    "\n",
    "# Similar wrapper for scrape_website\n",
    "def scrape_website_with_timeout(url, timeout=60):\n",
    "    result = [None]\n",
    "    def job():\n",
    "        result[0] = scrape_website(url)\n",
    "    thread = threading.Thread(target=job)\n",
    "    thread.start()\n",
    "    thread.join(timeout=timeout)\n",
    "    if thread.is_alive():\n",
    "        logging.error(f\"Timeout: scrape_website took longer than {timeout} seconds.\")\n",
    "        return None\n",
    "    else:\n",
    "        return result[0]\n",
    "\n",
    "# Prompt the user for the number of links to process\n",
    "num_links = int(input(\"Enter the number of links to process: \"))\n",
    "\n",
    "# Iterate over the DataFrame\n",
    "for index, row in df.head(num_links).iterrows():\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing link {index+1} of {num_links}...\")\n",
    "    link = google_search_with_timeout(row)\n",
    "    if link is None:  # Skip this row if google_search took too long\n",
    "        continue\n",
    "    print(f\"Found link: {link} (Time taken: {time.time() - start_time} seconds)\")\n",
    "    df.loc[index, 'Link'] = link\n",
    "    start_time = time.time()\n",
    "    contact_info = scrape_website_with_timeout(link)\n",
    "    if contact_info is None:  # Skip this row if scrape_website took too long\n",
    "        continue\n",
    "    print(f\"Found contact info: {contact_info} (Time taken: {time.time() - start_time} seconds)\")\n",
    "    email_matches = ', '.join(contact_info[0]) if contact_info[0] else guess_emails(row['Legal Business Name'])\n",
    "    df.loc[index, 'Emails'] = email_matches\n",
    "    df.loc[index, 'Phone Numbers'] = ', '.join(contact_info[1]) if contact_info[1] else ''\n",
    "    df.loc[index, 'Contact Form Link'] = contact_info[2] if contact_info[2] else ''\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# Save the DataFrame back to an xls file\n",
    "df.to_excel('schoolInfo.xlsx', index=False)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the excel file\n",
    "df = pd.read_excel('first5000.xlsx')\n",
    "\n",
    "# Function to clean the business name\n",
    "def clean_business_name(business_name):\n",
    "    if isinstance(business_name, str):\n",
    "        # Convert to lower case, remove spaces and special characters\n",
    "        business_name = re.sub(r'\\W+', '', business_name).lower()\n",
    "    else:\n",
    "        business_name = ''\n",
    "    return business_name\n",
    "\n",
    "# Function to clean the email\n",
    "def clean_email(email):\n",
    "    # Split by comma in case of multiple emails\n",
    "    emails = email.split(\", \")\n",
    "    cleaned_emails = []\n",
    "    for email in emails:\n",
    "        # Find the first occurrence of a letter and the last occurrence of a period\n",
    "        match = re.search(r'[a-zA-Z].*?\\.[a-zA-Z]+', email)\n",
    "        if match:\n",
    "            # If a match is found, extract it and add to cleaned_emails\n",
    "            cleaned_emails.append(match.group())\n",
    "    # Join cleaned_emails into a string, separated by commas\n",
    "    cleaned_email_str = \", \".join(cleaned_emails)\n",
    "    return cleaned_email_str\n",
    "\n",
    "# Apply clean_business_name function to the 'Legal Business Name' column\n",
    "df['Legal Business Name'] = df['Legal Business Name'].apply(clean_business_name)\n",
    "\n",
    "# Iterate over the rows of the dataframe\n",
    "for i, row in df.iterrows():\n",
    "    # If Emails is empty, fill with guessed emails\n",
    "    if pd.isnull(row['Emails']):\n",
    "        business_name = row['Legal Business Name']\n",
    "        guessed_emails = f\"{business_name}@gmail.com, info@{business_name}.com\"\n",
    "        df.loc[i, 'Emails'] = guessed_emails\n",
    "    else:\n",
    "        # Check for duplicate emails\n",
    "        email_list = row['Emails'].split(\", \")\n",
    "        # Remove duplicates by converting the list to a set, then convert back to list\n",
    "        email_list = list(set(email_list))\n",
    "        # Join the emails back into a string, separated by commas\n",
    "        email_str = \", \".join(email_list)\n",
    "        # Replace the email cell with the cleaned emails\n",
    "        df.loc[i, 'Emails'] = email_str\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Apply clean_email function to 'Emails' column\n",
    "df_cleaned['Emails'] = df_cleaned['Emails'].apply(clean_email)\n",
    "\n",
    "# Save the cleaned dataframe to a new excel file\n",
    "df_cleaned.to_excel('cleanedEmails5000.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e41b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('cleanedEmails5000.xlsx')\n",
    "\n",
    "# Select the third column. Note that Python uses 0-indexing, so the third column would be index 2.\n",
    "third_column = df.iloc[:, 2]\n",
    "\n",
    "# Save the third column to a CSV file\n",
    "third_column.to_csv('third_column.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bec436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open the input and output files\n",
    "with open('third_column.csv', 'r') as infile, open('output.csv', 'w', newline='') as outfile:\n",
    "    # Create a CSV writer for the output file\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # Read the input file line by line\n",
    "    for line in infile:\n",
    "        # Split the line into emails\n",
    "        emails = line.strip().split(',')\n",
    "\n",
    "        # Write each email to the output file\n",
    "        for email in emails:\n",
    "            writer.writerow([email.strip()])  # strip() is used to remove leading/trailing whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650240c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# Regular expression pattern for matching email addresses\n",
    "pattern = re.compile(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)')\n",
    "\n",
    "# Open the input and output files\n",
    "with open('output.csv', 'r') as infile, open('cleaned_output.csv', 'w', newline='') as outfile:\n",
    "    # Create a CSV writer for the output file\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # Read the input file line by line\n",
    "    for line in infile:\n",
    "        # Remove quotes\n",
    "        line = line.replace('\"', '')\n",
    "\n",
    "        # Find the email address in the line\n",
    "        match = pattern.search(line)\n",
    "\n",
    "        # If an email address was found, write it to the output file\n",
    "        if match:\n",
    "            writer.writerow([match.group(1)])\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
